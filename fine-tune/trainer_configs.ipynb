{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0000c-81b1-476f-af8d-394f7c57479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "lora_r = 8 # 'lora_r' is the dimension of the LoRA attention. 8 for GPT-2\n",
    "lora_alpha = 16 # 'lora_alpha' is the alpha parameter for LoRA scaling.\n",
    "lora_dropout = 0.05 # 'lora_dropout' is the dropout probability for LoRA layers.\n",
    "target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]  # GPT-2 specific target modules\n",
    "\n",
    "# attn.q_proj\n",
    "# attn.k_proj\n",
    "# attn.v_proj\n",
    "# attn.c_proj\n",
    "# mlp.c_fc\n",
    "# mlp.c_proj\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer..\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",  # GPT-2 doesn't use bias in attention layers\n",
    ")\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 1, meaning that the batch size for training and evaluation will be 4 per device.\n",
    "# 'gradient_accumulation_steps' is set to 8, meaning that gradients will be accumulated over 8 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"info\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 5e-5, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device and the model.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device and the model..\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 200, meaning that the model will be trained for 200 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"cosine\", meaning that a cosine learning rate scheduler will be used.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-LoRA-nl-to-fol-2\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=8, \n",
    "    per_device_eval_batch_size=1,  \n",
    "    log_level=\"info\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100, \n",
    "    learning_rate=5e-5, \n",
    "    fp16=True,  \n",
    "    eval_steps=100,  \n",
    "    num_train_epochs=200, \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# 'model' is the model that will be trained.\n",
    "# 'train_dataset' and 'eval_dataset' are the datasets that will be used for training and evaluation, respectively.\n",
    "# 'peft_config' is the configuration for peft, which is used for instruction tuning.\n",
    "# 'dataset_text_field' is set to \"text\", meaning that the 'text' field of the dataset will be used as the input for the model.\n",
    "# 'max_seq_length' is set to 256, meaning that the maximum length of the sequences that will be fed to the model is 256 tokens.\n",
    "# 'tokenizer' is the tokenizer that will be used to tokenize the input text.\n",
    "# 'args' are the training arguments that were defined earlier.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256, \n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
