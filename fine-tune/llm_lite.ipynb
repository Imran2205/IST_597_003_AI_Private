{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from typing import Any, List, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from langchain.llms.base import LLM\n",
    "import logging\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain import llms\n",
    "import inspect\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to force the code to only use CPU, uncomment the following line\n",
    "# torch.cuda.is_available = lambda : False\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "from typing import Any, List, Optional, Union\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "class GPT2LLM(LLM):\n",
    "    \"\"\"\n",
    "    A custom Language Model class that wraps GPT-2 or LoRA-fine-tuned GPT-2 models.\n",
    "    This class extends LangChain's LLM class to provide integration with GPT-2 models,\n",
    "    including support for LoRA-adapted models.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = Field(default=\"gpt2\", description=\"Name of the base model to use\")\n",
    "    model_path: str = Field(default=None, description=\"Path to the fine-tuned or LoRA model\")\n",
    "    model: Union[GPT2LMHeadModel, PeftModel] = Field(default=None, description=\"The loaded model\")\n",
    "    tokenizer: GPT2Tokenizer = Field(default=None, description=\"The tokenizer for the model\")\n",
    "    use_lora: bool = Field(default=False, description=\"Whether to use a LoRA-adapted model\")\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt2\", model_path: str = None, use_lora: bool = False, **data: Any):\n",
    "        \"\"\"\n",
    "        Initialize the GPT2LLM instance.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the base model to use. Defaults to \"gpt2\".\n",
    "            model_path (str, optional): Path to the fine-tuned or LoRA model. Defaults to None.\n",
    "            use_lora (bool): Whether to use a LoRA-adapted model. Defaults to False.\n",
    "            **data: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__(**data)\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "        self.use_lora = use_lora\n",
    "        self.model, self.tokenizer = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the model and tokenizer based on the provided configuration.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the loaded model and tokenizer.\n",
    "        \"\"\"\n",
    "        if self.model_path:\n",
    "            if self.use_lora:\n",
    "                # Load the LoRA model\n",
    "                config = PeftConfig.from_pretrained(self.model_path)\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "                model = PeftModel.from_pretrained(base_model, self.model_path).to(device)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            else:\n",
    "                # Load a regular fine-tuned model\n",
    "                model = GPT2LMHeadModel.from_pretrained(self.model_path).to(device)\n",
    "                tokenizer = GPT2Tokenizer.from_pretrained(self.model_path)\n",
    "        else:\n",
    "            # Load the default model\n",
    "            model = GPT2LMHeadModel.from_pretrained(self.model_name).to(device)\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Set the pad token to the eos token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate text based on the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt for text generation.\n",
    "            stop (Optional[List[str]]): A list of strings to stop generation when encountered. Not used in this implementation.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated text.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        output = self.model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=250,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the identifying parameters of the model.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the model's identifying parameters.\n",
    "        \"\"\"\n",
    "        return {\"model_name\": self.model_name, \"model_path\": self.model_path, \"use_lora\": self.use_lora}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the type of the language model.\n",
    "\n",
    "        Returns:\n",
    "            str: The type of the language model, which is \"GPT2\" in this case.\n",
    "        \"\"\"\n",
    "        return \"GPT2\"\n",
    "\n",
    "def setup_custom_llm(model_path: str = None, use_lora: bool = False):\n",
    "    \"\"\"\n",
    "    Set up a custom GPT2LLM instance.\n",
    "\n",
    "    Args:\n",
    "        model_path (str, optional): Path to the fine-tuned or LoRA model. Defaults to None.\n",
    "        use_lora (bool): Whether to use a LoRA-adapted model. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        GPT2LLM: An instance of the GPT2LLM class.\n",
    "    \"\"\"\n",
    "    return GPT2LLM(model_path=model_path, use_lora=use_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LangChain Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM and a simple interface\n",
    "\n",
    "llm = setup_custom_llm()\n",
    "\n",
    "template = \"\"\"You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
    "Given the following query, provide the corresponding FOL translation:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
    "\n",
    "FOL Translation: [Your FOL statement here]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
    "\n",
    "# Create a chain using the new LangChain method\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Query: Is there any car with speed over 60mph?\n",
      "\n",
      "GPT-2 Response:\n",
      "You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: Is there any car with speed over 60mph?\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_ERROR_CODE option to disable this option.)\n",
      ", and, if you have a query that does not have the same FOB as the query you provided, use this query: Query: Does the car have speed above 60 mph? (This is the default value.) (If you do not specify this, the result will be a FOUND_RANGE_VALUE value that will not be returned.) Query : Is the vehicle in the range of 60 to 60 miles? If so, return the value of the current range. (The default is 60.) If not specified, this will return a value in range 0 to 100. Query is a function that returns a result\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def testLLMLite():\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"Is there any car with speed over 60mph?\",\n",
    "        # \"Are all students in the class older than 18?\",\n",
    "        # \"Does there exist a book that is both educational and entertaining?\",\n",
    "        # \"Are there at least two different colors of flowers in the garden?\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nOriginal Query: {query}\")\n",
    "        result = chain.invoke(query)\n",
    "        print(\"\\nGPT-2 Response:\")\n",
    "        print(result)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# test it\n",
    "testLLMLite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7862/startup-events \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7862/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:FOL: You are an AI assistant specialized in translating natural language queries into First-Order Logic (FOL) statements. \n",
      "Given the following query, provide the corresponding FOL translation:\n",
      "\n",
      "Query: Is there any car with speed over 60mph?\n",
      "\n",
      "Translate the above query into a First-Order Logic statement. Your response should follow this format:\n",
      "\n",
      "FOL Translation: [Your FOL statement here]\n",
      ".\n",
      " (Note: If you are using a language that is not FOO, you can use the FOCAL_FULL_ERROR_CODE option to disable this option.)\n",
      ", and, if you have a query that does not have the same FOB as the query you provided, use this query: Query: Does the car have speed above 60 mph? (This is the default value.) (If you do not specify this, the result will be a FOUND_RANGE_VALUE value that will not be returned.) Query : Is the vehicle in the range of 60 to 60 miles? If so, return the value of the current range. (The default is 60.) If not specified, this will return a value in range 0 to 100. Query is a function that returns a result\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def testLLMLite(question):    \n",
    "    try:\n",
    "        result = chain.invoke(question)\n",
    "        logger.info(f\"FOL: {result}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "ui = gr.Interface(\n",
    "    fn=testLLMLite,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"FOL generator\",\n",
    "    description=\"Enter the question in natural language\",\n",
    "    examples=[\n",
    "        [\"Is there any car with speed over 60mph?\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the interface without rendering in the notebook\n",
    "ui.launch(share=False, inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
