{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717853f4-726e-497c-9a46-d80f357b3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentOutputParser \n",
    "from typing import List, Union\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from rouge import Rouge\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22671c-1ef7-4cb8-883e-d3987930f09b",
   "metadata": {},
   "source": [
    "## Get the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d4430-bce7-4351-9217-3a6fd43d9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to register and get API key\n",
    "def register_user():\n",
    "    register_url = \"http://127.0.0.1:8899/v1/register\"\n",
    "    try:\n",
    "        response = requests.post(register_url)\n",
    "        response.raise_for_status()\n",
    "        api_key = response.json()[\"api_key\"]\n",
    "        logger.info(\"Successfully registered and received API key\")\n",
    "        return api_key\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to register user: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Get or create API key\n",
    "try:\n",
    "    with open(\"api_key.txt\", \"r\") as f:\n",
    "        api_key = f.read().strip()\n",
    "    logger.info(\"Loaded existing API key\")\n",
    "except FileNotFoundError:\n",
    "    api_key = register_user()\n",
    "    with open(\"api_key.txt\", \"w\") as f:\n",
    "        f.write(api_key)\n",
    "    logger.info(\"Registered new user and saved API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f38cb-7434-43c0-a989-45f209311e06",
   "metadata": {},
   "source": [
    "## Custom LLM\n",
    "- Same as Project 1\n",
    "- Only difference is the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b2758-5490-470d-b585-b230baaa1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    api_url: str = \"http://127.0.0.1:8899/v1/completions\"\n",
    "    api_key: str = None\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__()\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "    ):\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-API-Key\": self.api_key\n",
    "        }\n",
    "        data = {\n",
    "            \"prompt\": prompt + \"\\nAnswer:\",\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 1.0,\n",
    "            \"n\": 1,\n",
    "            \"stop\": stop or [\"Human:\", \"\\n\\n\"]\n",
    "        }\n",
    "        try:\n",
    "            logger.info(f\"Sending prompt to API: {prompt}\")\n",
    "            response = requests.post(self.api_url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()['choices'][0]['text']\n",
    "            logger.info(f\"Received response from API: {result}\")\n",
    "            return result.strip()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"API request failed: {str(e)}\")\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"Unexpected API response format: {str(e)}\")\n",
    "            return f\"Sorry, I received an unexpected response format: {str(e)}\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "# Initialize the custom LLM\n",
    "llm = CustomLLM(api_key=api_key)\n",
    "\n",
    "logger.info(\"Custom LLM initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731ba29-aa43-4d8c-9e48-6cf041b9d527",
   "metadata": {},
   "source": [
    "## FOL translator function\n",
    "- Uses the fine-tuned GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a1607-3e87-4605-bd1e-88177f151966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "model_path = \"./gpt-2-nl-to-fol-merged\"\n",
    "logger.info(f\"Loading GPT-2 model from path: {model_path}\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "logger.info(\"GPT-2 model and tokenizer loaded successfully\")\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "logger.info(\"Padding token set for GPT-2 model\")\n",
    "\n",
    "# GPT-2 Tool\n",
    "def gpt2_fol_translation(question: str) -> str:\n",
    "    logger.info(f\"Translating to FOL using GPT-2: {question}\")\n",
    "    SYSTEM_PROMPT = \"Translate the following natural language question to First Order Logic (FOL). Please respond with only the FOL statement. Don't include additional text.\\nQuestion: \"\n",
    "    full_input = SYSTEM_PROMPT + question + \"\\nFOL Query:\"\n",
    "    input_ids = tokenizer.encode(full_input, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=150, num_return_sequences=1, temperature=0.7)\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    fol_output = generated_text.split(\"FOL Query:\")[-1].strip()\n",
    "    logger.info(f\"GPT-2 FOL translation: {fol_output}\")\n",
    "    return fol_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed7e70-3573-4765-b49b-acd35caa3237",
   "metadata": {},
   "source": [
    "## Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23694092-674f-4b7f-aa12-c4e93c151f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge Tool\n",
    "\n",
    "def call_judge(question: str) -> str:\n",
    "    logger.info(f\"Translating to FOL using Judge: {question}\")\n",
    "    api_url = \"http://127.0.0.1:8899/v1/judge\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-API-Key\": api_key\n",
    "    }\n",
    "    data = {\n",
    "        \"prompt\": f\"{question}\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        result = response.json()[\"choices\"][0][\"text\"].strip()\n",
    "        logger.info(f\"Judge FOL translation: {result}\")\n",
    "        return result\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Judge API request failed: {str(e)}\")\n",
    "        return f\"Error: Unable to get response from judge API\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0b4d7-bceb-47df-8522-e46fe35594c7",
   "metadata": {},
   "source": [
    "## Define the tool list according to your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3d15d-23a0-4cfc-b620-49c3b5b6181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    # Populate with your tool list\n",
    "]\n",
    "logger.info(\"Tools defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f26b1-be34-48d0-a51f-869ca4d8d994",
   "metadata": {},
   "source": [
    "## Update the `CustomPromptTemplate` class and `prompt_template` for using proper tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106dc23-8370-4afc-b2e5-cd7344b881bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format(self, **kwargs):\n",
    "        # Populate with your custom formatting code\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "logger.info(\"CustomPromptTemplate set up\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Your instruction prompt\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Prompt template defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4357ca-395b-4209-bdcb-c084180c480a",
   "metadata": {},
   "source": [
    "## Custom Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c7e43-a94b-444d-8e2f-7c5e3bd5bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output parser\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str):\n",
    "        logger.debug(f\"+++++ Parsing LLM output: {llm_output}\")\n",
    "        if \"[END_OF_RESPONSE]\" in llm_output:\n",
    "            response = llm_output.split(\"[END_OF_RESPONSE]\")[0].strip()\n",
    "\n",
    "            if response == \"\":\n",
    "                response = llm_output.split(\"[END_OF_RESPONSE]\")[1].strip()\n",
    "            \n",
    "            # Extract only the Final Answer\n",
    "            if \"Final Answer:\" in response:\n",
    "                final_answer = response.split(\"Final Answer:\")[-1].strip()\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": final_answer},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "            else:\n",
    "                return AgentFinish(\n",
    "                    return_values={\"output\": response},\n",
    "                    log=llm_output,\n",
    "                )\n",
    "\n",
    "        # Sanity Check: Check if this is the final answer\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            final_answer = llm_output.split(\"Final Answer:\")[-1].strip()\n",
    "            if \"[END_OF_RESPONSE]\" in final_answer:\n",
    "                final_answer = final_answer.split(\"[END_OF_RESPONSE]\")[0].strip()\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": final_answer},\n",
    "                log=llm_output,\n",
    "            )\n",
    "            \n",
    "        pattern = r\"Action: (.*?)\\nAction Input: (.*?)(?=\\n|$)\"\n",
    "        match = re.search(pattern, llm_output, re.DOTALL)\n",
    "\n",
    "        if not match:\n",
    "            # return llm_output\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": f\"I apologize, but I encountered an error. Please try again or rephrase your message.\"},\n",
    "                log=llm_output,\n",
    "            )\n",
    "            # raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "            \n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2).strip()\n",
    "        \n",
    "        logger.debug(f\"------ Parsed action: {action}, action input: {action_input}\")\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "logger.info(\"CustomOutputParser defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10083e5d-6f37-4b53-977a-f9cbcff0d9a7",
   "metadata": {},
   "source": [
    "## Initialize prompt template and output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3f766-3965-4160-98e9-86380c454b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=prompt_template,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")\n",
    "\n",
    "output_parser = CustomOutputParser()\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "tool_names = [tool.name for tool in tools]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94932700-16e2-49f6-8bfe-34fb2118644f",
   "metadata": {},
   "source": [
    "## Creating Agent that can use tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63602ae4-dccf-4b7c-b18f-704564878092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")\n",
    "logger.info(\"Agent initialized\")\n",
    "\n",
    "# Set up the agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, max_iterations=10)\n",
    "logger.info(\"Agent executor set up\")\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "logger.info(\"Conversation memory initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29424975-65ab-47ca-a415-a54ad3f5393c",
   "metadata": {},
   "source": [
    "## define you chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2d976-e95b-4e0d-83c3-10231611ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio chat interface backend\n",
    "def chat(message, history):\n",
    "    # Add necessary code for the chat\n",
    "    response = message\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a357f-a510-4408-8c54-099da423d72f",
   "metadata": {},
   "source": [
    "## Modify the the gradio interface as required\n",
    "- The version below adds a `Verify` button, which, when pressed, sends a `Verify` message in the chat.\n",
    "- You can modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f6a92-2d48-4597-9a83-ef55f0e6ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CSS for full height\n",
    "custom_css = \"\"\"\n",
    "#chatbot-container {\n",
    "    height: calc(100vh - 230px) !important;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "#input-container {\n",
    "    position: fixed;\n",
    "    bottom: 0;\n",
    "    left: 0;\n",
    "    right: 0;\n",
    "    padding: 20px;\n",
    "    background-color: white;\n",
    "    border-top: 1px solid #ccc;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"Custom CSS defined\")\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=custom_css) as gr_iface:\n",
    "    with gr.Column():\n",
    "        chatbot = gr.Chatbot(elem_id=\"chatbot-container\")\n",
    "        with gr.Row(elem_id=\"input-container\"):\n",
    "            msg = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n",
    "            send = gr.Button(\"Send\")\n",
    "            with gr.Row():\n",
    "                verify = gr.Button(\"Verify\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "            \n",
    "    def user(user_message, history):\n",
    "        logger.debug(f\"User message: {user_message}\")\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        user_message = history[-1][0]\n",
    "        logger.debug(f\"Processing bot response for: {user_message}\")\n",
    "        bot_message = chat(user_message, history[:-1])\n",
    "        history[-1][1] = bot_message\n",
    "        return history\n",
    "\n",
    "    def clear_chat():\n",
    "        return None\n",
    "\n",
    "    def verify_click(history):\n",
    "        return user(\"Verify\", history)[1]\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    send.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(clear_chat, None, chatbot, queue=False)\n",
    "    verify.click(verify_click, chatbot, chatbot).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "logger.info(\"Gradio interface created\")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "logger.info(\"Launching Gradio interface\")\n",
    "gr_iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
