{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune a Generative AI Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n",
    "  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n",
    "  - [ 1.2 - Load Dataset and LLM](#1.2)\n",
    "  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
    "- [ 2 - Perform Full Fine-Tuning](#2)\n",
    "  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n",
    "  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
    "  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
    "  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n",
    "- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n",
    "  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n",
    "  - [ 3.2 - Train PEFT Adapter](#3.2)\n",
    "  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n",
    "  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Kernel, Load Required Dependencies, Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \\\n",
    "#     torch\n",
    "#     torchdata\n",
    "#     transformers \\\n",
    "#     datasets \\\n",
    "#     evaluate \\\n",
    "#     rouge_score \\\n",
    "#     loralib \\\n",
    "#     peft \\\n",
    "#     trl \\\n",
    "#     awscli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import \\\n",
    "    load_dataset, \\\n",
    "    Dataset\n",
    "\n",
    "from transformers import \\\n",
    "        AutoModelForSeq2SeqLM, \\\n",
    "        AutoTokenizer, \\\n",
    "        GenerationConfig, \\\n",
    "        TrainingArguments, \\\n",
    "        Trainer, \\\n",
    "        AutoModelForCausalLM\n",
    "\n",
    "from trl import \\\n",
    "    SFTConfig, \\\n",
    "    SFTTrainer\n",
    "\n",
    "from peft import \\\n",
    "    LoraConfig, \\\n",
    "    TaskType, \\\n",
    "    PeftModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify which GPU to use (0, 1, or 2)\n",
    "# gpu_id = 0\n",
    "# device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# force the code to only use CPU\n",
    "# torch.cuda.is_available = lambda : False\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the specified device\n",
    "# original_model = original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121 cpu\n"
     ]
    }
   ],
   "source": [
    "#version check\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset from Huggingface\n",
    "\n",
    "You are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name, split='train')\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "# show summary\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from Local Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(file_path, train_size=0.95, random_state=42):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    df['text'] = df.apply(lambda row: f\"Translate the following natural language question to First Order Logic (FOL). Please respond with only the FOL statement. Don't include additional text.\\nQuestion: {row['Question']}\\nFOL Query: {row['FOL Query']}\", axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    return {\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df)\n",
    "    }\n",
    "#end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['Question', 'FOL Query', 'text', '__index_level_0__'],\n",
       "     num_rows: 438\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['Question', 'FOL Query', 'text', '__index_level_0__'],\n",
       "     num_rows: 24\n",
       " })}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_fol = load_csv_dataset(\"question_query_train.csv\")\n",
    "display(dataset_fol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an LLM from Huggingface for Seq2Seq tasks\n",
    "It requires loading the model and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading a medium model: T-5\n",
    "model_name_1='google/flan-t5-base'\n",
    "original_model_t5 = AutoModelForSeq2SeqLM.from_pretrained(model_name_1, torch_dtype=torch.bfloat16)\n",
    "original_model_t5.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an LLM for Causal tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_2='gpt2'\n",
    "# device_map = 'auto'\n",
    "\n",
    "original_model_gpt2 = AutoModelForCausalLM.from_pretrained(model_name_2, trust_remote_code=True)\n",
    "original_model_gpt2.to(device)\n",
    "\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name_2)\n",
    "tokenizer_gpt2.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify what weights to tune\n",
    "\n",
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    # numel stands for \"number of elements\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def identify_trainable_layers(model):\n",
    "    trainable_layers = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_layers.append(name)\n",
    "    return trainable_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 247577856 for T5\n",
      "Total parameters: 247577856\n",
      "Percentage of trainable parameters: 100.00%\n",
      "Trainable layers:\n",
      "- shared.weight\n",
      "- encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "- encoder.block.0.layer.0.layer_norm.weight\n",
      "- encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.0.layer.1.layer_norm.weight\n",
      "- encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.1.layer.0.layer_norm.weight\n",
      "- encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.1.layer.1.layer_norm.weight\n",
      "- encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.2.layer.0.layer_norm.weight\n",
      "- encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.2.layer.1.layer_norm.weight\n",
      "- encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.3.layer.0.layer_norm.weight\n",
      "- encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.3.layer.1.layer_norm.weight\n",
      "- encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.4.layer.0.layer_norm.weight\n",
      "- encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.4.layer.1.layer_norm.weight\n",
      "- encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.5.layer.0.layer_norm.weight\n",
      "- encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.5.layer.1.layer_norm.weight\n",
      "- encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.6.layer.0.layer_norm.weight\n",
      "- encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.6.layer.1.layer_norm.weight\n",
      "- encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.7.layer.0.layer_norm.weight\n",
      "- encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.7.layer.1.layer_norm.weight\n",
      "- encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.8.layer.0.layer_norm.weight\n",
      "- encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.8.layer.1.layer_norm.weight\n",
      "- encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.9.layer.0.layer_norm.weight\n",
      "- encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.9.layer.1.layer_norm.weight\n",
      "- encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.10.layer.0.layer_norm.weight\n",
      "- encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.10.layer.1.layer_norm.weight\n",
      "- encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "- encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "- encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "- encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "- encoder.block.11.layer.0.layer_norm.weight\n",
      "- encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "- encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "- encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "- encoder.block.11.layer.1.layer_norm.weight\n",
      "- encoder.final_layer_norm.weight\n",
      "- decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "- decoder.block.0.layer.0.layer_norm.weight\n",
      "- decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.0.layer.1.layer_norm.weight\n",
      "- decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.0.layer.2.layer_norm.weight\n",
      "- decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.1.layer.0.layer_norm.weight\n",
      "- decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.1.layer.1.layer_norm.weight\n",
      "- decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.1.layer.2.layer_norm.weight\n",
      "- decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.2.layer.0.layer_norm.weight\n",
      "- decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.2.layer.1.layer_norm.weight\n",
      "- decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.2.layer.2.layer_norm.weight\n",
      "- decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.3.layer.0.layer_norm.weight\n",
      "- decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.3.layer.1.layer_norm.weight\n",
      "- decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.3.layer.2.layer_norm.weight\n",
      "- decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.4.layer.0.layer_norm.weight\n",
      "- decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.4.layer.1.layer_norm.weight\n",
      "- decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.4.layer.2.layer_norm.weight\n",
      "- decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.5.layer.0.layer_norm.weight\n",
      "- decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.5.layer.1.layer_norm.weight\n",
      "- decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.5.layer.2.layer_norm.weight\n",
      "- decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.6.layer.0.layer_norm.weight\n",
      "- decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.6.layer.1.layer_norm.weight\n",
      "- decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.6.layer.2.layer_norm.weight\n",
      "- decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.7.layer.0.layer_norm.weight\n",
      "- decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.7.layer.1.layer_norm.weight\n",
      "- decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.7.layer.2.layer_norm.weight\n",
      "- decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.8.layer.0.layer_norm.weight\n",
      "- decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.8.layer.1.layer_norm.weight\n",
      "- decoder.block.8.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.8.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.8.layer.2.layer_norm.weight\n",
      "- decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.9.layer.0.layer_norm.weight\n",
      "- decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.9.layer.1.layer_norm.weight\n",
      "- decoder.block.9.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.9.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.9.layer.2.layer_norm.weight\n",
      "- decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.10.layer.0.layer_norm.weight\n",
      "- decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.10.layer.1.layer_norm.weight\n",
      "- decoder.block.10.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.10.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.10.layer.2.layer_norm.weight\n",
      "- decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "- decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "- decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "- decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "- decoder.block.11.layer.0.layer_norm.weight\n",
      "- decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "- decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "- decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "- decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "- decoder.block.11.layer.1.layer_norm.weight\n",
      "- decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "- decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "- decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "- decoder.block.11.layer.2.layer_norm.weight\n",
      "- decoder.final_layer_norm.weight\n",
      "- lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Count trainable parameters\n",
    "trainable_params = count_trainable_parameters(original_model_t5)\n",
    "print(f\"Total trainable parameters: {trainable_params} for T5\")\n",
    "\n",
    "# Identify trainable layers\n",
    "trainable_layers = identify_trainable_layers(original_model_t5)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = sum(p.numel() for p in original_model_t5.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
    "\n",
    "print(\"Trainable layers:\")\n",
    "for layer in trainable_layers:\n",
    "    print(f\"- {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 124439808 for GPT2\n",
      "Total parameters: 124439808\n",
      "Percentage of trainable parameters: 100.00%\n",
      "Trainable layers:\n",
      "- transformer.wte.weight\n",
      "- transformer.wpe.weight\n",
      "- transformer.h.0.ln_1.weight\n",
      "- transformer.h.0.ln_1.bias\n",
      "- transformer.h.0.attn.c_attn.weight\n",
      "- transformer.h.0.attn.c_attn.bias\n",
      "- transformer.h.0.attn.c_proj.weight\n",
      "- transformer.h.0.attn.c_proj.bias\n",
      "- transformer.h.0.ln_2.weight\n",
      "- transformer.h.0.ln_2.bias\n",
      "- transformer.h.0.mlp.c_fc.weight\n",
      "- transformer.h.0.mlp.c_fc.bias\n",
      "- transformer.h.0.mlp.c_proj.weight\n",
      "- transformer.h.0.mlp.c_proj.bias\n",
      "- transformer.h.1.ln_1.weight\n",
      "- transformer.h.1.ln_1.bias\n",
      "- transformer.h.1.attn.c_attn.weight\n",
      "- transformer.h.1.attn.c_attn.bias\n",
      "- transformer.h.1.attn.c_proj.weight\n",
      "- transformer.h.1.attn.c_proj.bias\n",
      "- transformer.h.1.ln_2.weight\n",
      "- transformer.h.1.ln_2.bias\n",
      "- transformer.h.1.mlp.c_fc.weight\n",
      "- transformer.h.1.mlp.c_fc.bias\n",
      "- transformer.h.1.mlp.c_proj.weight\n",
      "- transformer.h.1.mlp.c_proj.bias\n",
      "- transformer.h.2.ln_1.weight\n",
      "- transformer.h.2.ln_1.bias\n",
      "- transformer.h.2.attn.c_attn.weight\n",
      "- transformer.h.2.attn.c_attn.bias\n",
      "- transformer.h.2.attn.c_proj.weight\n",
      "- transformer.h.2.attn.c_proj.bias\n",
      "- transformer.h.2.ln_2.weight\n",
      "- transformer.h.2.ln_2.bias\n",
      "- transformer.h.2.mlp.c_fc.weight\n",
      "- transformer.h.2.mlp.c_fc.bias\n",
      "- transformer.h.2.mlp.c_proj.weight\n",
      "- transformer.h.2.mlp.c_proj.bias\n",
      "- transformer.h.3.ln_1.weight\n",
      "- transformer.h.3.ln_1.bias\n",
      "- transformer.h.3.attn.c_attn.weight\n",
      "- transformer.h.3.attn.c_attn.bias\n",
      "- transformer.h.3.attn.c_proj.weight\n",
      "- transformer.h.3.attn.c_proj.bias\n",
      "- transformer.h.3.ln_2.weight\n",
      "- transformer.h.3.ln_2.bias\n",
      "- transformer.h.3.mlp.c_fc.weight\n",
      "- transformer.h.3.mlp.c_fc.bias\n",
      "- transformer.h.3.mlp.c_proj.weight\n",
      "- transformer.h.3.mlp.c_proj.bias\n",
      "- transformer.h.4.ln_1.weight\n",
      "- transformer.h.4.ln_1.bias\n",
      "- transformer.h.4.attn.c_attn.weight\n",
      "- transformer.h.4.attn.c_attn.bias\n",
      "- transformer.h.4.attn.c_proj.weight\n",
      "- transformer.h.4.attn.c_proj.bias\n",
      "- transformer.h.4.ln_2.weight\n",
      "- transformer.h.4.ln_2.bias\n",
      "- transformer.h.4.mlp.c_fc.weight\n",
      "- transformer.h.4.mlp.c_fc.bias\n",
      "- transformer.h.4.mlp.c_proj.weight\n",
      "- transformer.h.4.mlp.c_proj.bias\n",
      "- transformer.h.5.ln_1.weight\n",
      "- transformer.h.5.ln_1.bias\n",
      "- transformer.h.5.attn.c_attn.weight\n",
      "- transformer.h.5.attn.c_attn.bias\n",
      "- transformer.h.5.attn.c_proj.weight\n",
      "- transformer.h.5.attn.c_proj.bias\n",
      "- transformer.h.5.ln_2.weight\n",
      "- transformer.h.5.ln_2.bias\n",
      "- transformer.h.5.mlp.c_fc.weight\n",
      "- transformer.h.5.mlp.c_fc.bias\n",
      "- transformer.h.5.mlp.c_proj.weight\n",
      "- transformer.h.5.mlp.c_proj.bias\n",
      "- transformer.h.6.ln_1.weight\n",
      "- transformer.h.6.ln_1.bias\n",
      "- transformer.h.6.attn.c_attn.weight\n",
      "- transformer.h.6.attn.c_attn.bias\n",
      "- transformer.h.6.attn.c_proj.weight\n",
      "- transformer.h.6.attn.c_proj.bias\n",
      "- transformer.h.6.ln_2.weight\n",
      "- transformer.h.6.ln_2.bias\n",
      "- transformer.h.6.mlp.c_fc.weight\n",
      "- transformer.h.6.mlp.c_fc.bias\n",
      "- transformer.h.6.mlp.c_proj.weight\n",
      "- transformer.h.6.mlp.c_proj.bias\n",
      "- transformer.h.7.ln_1.weight\n",
      "- transformer.h.7.ln_1.bias\n",
      "- transformer.h.7.attn.c_attn.weight\n",
      "- transformer.h.7.attn.c_attn.bias\n",
      "- transformer.h.7.attn.c_proj.weight\n",
      "- transformer.h.7.attn.c_proj.bias\n",
      "- transformer.h.7.ln_2.weight\n",
      "- transformer.h.7.ln_2.bias\n",
      "- transformer.h.7.mlp.c_fc.weight\n",
      "- transformer.h.7.mlp.c_fc.bias\n",
      "- transformer.h.7.mlp.c_proj.weight\n",
      "- transformer.h.7.mlp.c_proj.bias\n",
      "- transformer.h.8.ln_1.weight\n",
      "- transformer.h.8.ln_1.bias\n",
      "- transformer.h.8.attn.c_attn.weight\n",
      "- transformer.h.8.attn.c_attn.bias\n",
      "- transformer.h.8.attn.c_proj.weight\n",
      "- transformer.h.8.attn.c_proj.bias\n",
      "- transformer.h.8.ln_2.weight\n",
      "- transformer.h.8.ln_2.bias\n",
      "- transformer.h.8.mlp.c_fc.weight\n",
      "- transformer.h.8.mlp.c_fc.bias\n",
      "- transformer.h.8.mlp.c_proj.weight\n",
      "- transformer.h.8.mlp.c_proj.bias\n",
      "- transformer.h.9.ln_1.weight\n",
      "- transformer.h.9.ln_1.bias\n",
      "- transformer.h.9.attn.c_attn.weight\n",
      "- transformer.h.9.attn.c_attn.bias\n",
      "- transformer.h.9.attn.c_proj.weight\n",
      "- transformer.h.9.attn.c_proj.bias\n",
      "- transformer.h.9.ln_2.weight\n",
      "- transformer.h.9.ln_2.bias\n",
      "- transformer.h.9.mlp.c_fc.weight\n",
      "- transformer.h.9.mlp.c_fc.bias\n",
      "- transformer.h.9.mlp.c_proj.weight\n",
      "- transformer.h.9.mlp.c_proj.bias\n",
      "- transformer.h.10.ln_1.weight\n",
      "- transformer.h.10.ln_1.bias\n",
      "- transformer.h.10.attn.c_attn.weight\n",
      "- transformer.h.10.attn.c_attn.bias\n",
      "- transformer.h.10.attn.c_proj.weight\n",
      "- transformer.h.10.attn.c_proj.bias\n",
      "- transformer.h.10.ln_2.weight\n",
      "- transformer.h.10.ln_2.bias\n",
      "- transformer.h.10.mlp.c_fc.weight\n",
      "- transformer.h.10.mlp.c_fc.bias\n",
      "- transformer.h.10.mlp.c_proj.weight\n",
      "- transformer.h.10.mlp.c_proj.bias\n",
      "- transformer.h.11.ln_1.weight\n",
      "- transformer.h.11.ln_1.bias\n",
      "- transformer.h.11.attn.c_attn.weight\n",
      "- transformer.h.11.attn.c_attn.bias\n",
      "- transformer.h.11.attn.c_proj.weight\n",
      "- transformer.h.11.attn.c_proj.bias\n",
      "- transformer.h.11.ln_2.weight\n",
      "- transformer.h.11.ln_2.bias\n",
      "- transformer.h.11.mlp.c_fc.weight\n",
      "- transformer.h.11.mlp.c_fc.bias\n",
      "- transformer.h.11.mlp.c_proj.weight\n",
      "- transformer.h.11.mlp.c_proj.bias\n",
      "- transformer.ln_f.weight\n",
      "- transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "trainable_params = count_trainable_parameters(original_model_gpt2)\n",
    "print(f\"Total trainable parameters: {trainable_params} for GPT2\")\n",
    "\n",
    "# Identify trainable layers\n",
    "trainable_layers = identify_trainable_layers(original_model_gpt2)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = sum(p.numel() for p in original_model_gpt2.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")\n",
    "\n",
    "print(\"Trainable layers:\")\n",
    "for layer in trainable_layers:\n",
    "    print(f\"- {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Naming Convention of Weights\n",
    "\n",
    "##### Key Components\n",
    "- **c_attn**: Prepares inputs for attention\n",
    "- **c_proj**: Processes the output of attention\n",
    "- **c_fc**: Part of the feed-forward network in each transformer block\n",
    "\n",
    "##### Historical Context\n",
    "The \"c\" prefix originates from the original OpenAI GPT (Generative Pre-trained Transformer) implementation, which was based on the codebase for the Transformer model introduced in the \"Attention Is All You Need\" paper.\n",
    "\n",
    "##### Meaning of \"c\"\n",
    "The \"c\" likely stands for \"convolution\" or \"conv\". This might seem odd at first, given that transformers don't use convolutions in the same way convolutional neural networks do. However, this terminology draws an analogy between attention mechanisms and dynamic convolutions.\n",
    "\n",
    "##### Special Note on c_attn\n",
    "`c_attn` is a merged representation of the Query (Q), Key (K), and Value (V) weight matrices. This is a key characteristic of the GPT-2 implementation that differentiates it from some other transformer architectures.\n",
    "\n",
    "##### Comparison with Standard Transformer Architecture\n",
    "1. **Standard Transformer:**\n",
    "   - Q, K, and V are separate linear projections:\n",
    "     - W_q for Query\n",
    "     - W_k for Key\n",
    "     - W_v for Value\n",
    "\n",
    "2. **GPT-2 Implementation:**\n",
    "   - These three projections are combined into a single larger matrix: `c_attn.weight`\n",
    "   - The matrix is effectively [W_q || W_k || W_v], where '||' represents concatenation\n",
    "\n",
    "##### Implications\n",
    "- This merged approach in GPT-2 can be more efficient in terms of computation and memory usage.\n",
    "- It allows for a single large matrix multiplication instead of three separate ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1 1\" width=\"800\" height=\"600\">  \n",
    "  <style>\n",
    "    .small { font: italic 0.0216666667px sans-serif; }\n",
    "    .heavy { font: bold 0.05px sans-serif; }\n",
    "    .caption { font: 0.025px sans-serif; }\n",
    "  </style>\n",
    "  \n",
    "  <!-- Input Embeddings -->\n",
    "  <rect x=\"0.0625\" y=\"0.0833333333\" width=\"0.25\" height=\"0.1333333333\" fill=\"#FFB3BA\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1875\" y=\"0.1583333333\" text-anchor=\"middle\" class=\"caption\">Input Embeddings</text>\n",
    "  <text x=\"0.1875\" y=\"0.1916666667\" text-anchor=\"middle\" class=\"small\">wte.weight, wpe.weight</text>\n",
    "  \n",
    "  <!-- Transformer Blocks -->\n",
    "  <rect x=\"0.0625\" y=\"0.25\" width=\"0.25\" height=\"0.6666666667\" fill=\"#BAFFC9\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1875\" y=\"0.2916666667\" text-anchor=\"middle\" class=\"caption\">Transformer Blocks</text>\n",
    "  \n",
    "  <!-- Block 0 -->\n",
    "  <rect x=\"0.0875\" y=\"0.3333333333\" width=\"0.2\" height=\"0.1666666667\" fill=\"#BAE1FF\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1875\" y=\"0.3666666667\" text-anchor=\"middle\" class=\"small\">Block 0 (h.0)</text>\n",
    "  <text x=\"0.1875\" y=\"0.4\" text-anchor=\"middle\" class=\"small\">ln_1, attn, ln_2, mlp</text>\n",
    "  <text x=\"0.1875\" y=\"0.4333333333\" text-anchor=\"middle\" class=\"small\">weights and biases</text>\n",
    "  \n",
    "  <!-- Block 1 -->\n",
    "  <rect x=\"0.0875\" y=\"0.5333333333\" width=\"0.2\" height=\"0.1666666667\" fill=\"#BAE1FF\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1875\" y=\"0.5666666667\" text-anchor=\"middle\" class=\"small\">Block 1 (h.1)</text>\n",
    "  <text x=\"0.1875\" y=\"0.6\" text-anchor=\"middle\" class=\"small\">ln_1, attn, ln_2, mlp</text>\n",
    "  <text x=\"0.1875\" y=\"0.6333333333\" text-anchor=\"middle\" class=\"small\">weights and biases</text>\n",
    "  \n",
    "  <!-- Block 2 -->\n",
    "  <rect x=\"0.0875\" y=\"0.7333333333\" width=\"0.2\" height=\"0.1666666667\" fill=\"#BAE1FF\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1875\" y=\"0.7666666667\" text-anchor=\"middle\" class=\"small\">Block 2 (h.2)</text>\n",
    "  <text x=\"0.1875\" y=\"0.8\" text-anchor=\"middle\" class=\"small\">ln_1, attn, ln_2, mlp</text>\n",
    "  <text x=\"0.1875\" y=\"0.8333333333\" text-anchor=\"middle\" class=\"small\">weights and biases</text>\n",
    "  \n",
    "  <!-- Legend -->\n",
    "  <rect x=\"0.375\" y=\"0.0833333333\" width=\"0.5625\" height=\"0.8333333333\" fill=\"#FFFFC9\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.65625\" y=\"0.1333333333\" text-anchor=\"middle\" class=\"caption\">Legend: Weight and Bias Descriptions</text>\n",
    "  \n",
    "  <text x=\"0.4\" y=\"0.1833333333\" class=\"small\">wte.weight: Token embedding weights</text>\n",
    "  <text x=\"0.4\" y=\"0.2166666667\" class=\"small\">wpe.weight: Position embedding weights</text>\n",
    "  <text x=\"0.4\" y=\"0.2666666667\" class=\"small\">For each block (h.0, h.1, h.2):</text>\n",
    "  <text x=\"0.425\" y=\"0.3\" class=\"small\">ln_1.weight, ln_1.bias: Layer norm 1</text>\n",
    "  <text x=\"0.425\" y=\"0.3333333333\" class=\"small\">attn.c_attn.weight, attn.c_attn.bias: Attention input</text>\n",
    "  <text x=\"0.425\" y=\"0.3666666667\" class=\"small\">attn.c_proj.weight, attn.c_proj.bias: Attention output</text>\n",
    "  <text x=\"0.425\" y=\"0.4\" class=\"small\">ln_2.weight, ln_2.bias: Layer norm 2</text>\n",
    "  <text x=\"0.425\" y=\"0.4333333333\" class=\"small\">mlp.c_fc.weight, mlp.c_fc.bias: MLP first layer</text>\n",
    "  <text x=\"0.425\" y=\"0.4666666667\" class=\"small\">mlp.c_proj.weight, mlp.c_proj.bias: MLP second layer</text>\n",
    "</svg>\n",
    "\n",
    "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1 1\" width=\"800\" height=\"600\">\n",
    "  <style>\n",
    "    .small { font: italic 0.0216666667px sans-serif; }\n",
    "    .caption { font: bold 0.0266666667px sans-serif; }\n",
    "    .subcaption { font: 0.0233333333px sans-serif; }\n",
    "  </style>\n",
    "  \n",
    "  <!-- Input -->\n",
    "  <rect x=\"0.0625\" y=\"0.0833333333\" width=\"0.125\" height=\"0.0666666667\" fill=\"#FFB3BA\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.125\" y=\"0.125\" text-anchor=\"middle\" class=\"subcaption\">Input</text>\n",
    "\n",
    "  <!-- Layer Norm 1 -->\n",
    "  <rect x=\"0.0625\" y=\"0.1833333333\" width=\"0.125\" height=\"0.1\" fill=\"#BAFFC9\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.125\" y=\"0.2333333333\" text-anchor=\"middle\" class=\"subcaption\">Layer Norm 1</text>\n",
    "  <text x=\"0.125\" y=\"0.2666666667\" text-anchor=\"middle\" class=\"small\">ln_1.weight</text>\n",
    "  <text x=\"0.125\" y=\"0.2916666667\" text-anchor=\"middle\" class=\"small\">ln_1.bias</text>\n",
    "\n",
    "  <!-- Self-Attention -->\n",
    "  <rect x=\"0.0625\" y=\"0.3166666667\" width=\"0.375\" height=\"0.3333333333\" fill=\"#BAE1FF\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.25\" y=\"0.3583333333\" text-anchor=\"middle\" class=\"caption\">Self-Attention</text>\n",
    "  \n",
    "  <!-- QKV Projection -->\n",
    "  <rect x=\"0.0875\" y=\"0.3833333333\" width=\"0.325\" height=\"0.1\" fill=\"#FFD700\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.25\" y=\"0.425\" text-anchor=\"middle\" class=\"subcaption\">QKV Projection</text>\n",
    "  <text x=\"0.25\" y=\"0.4583333333\" text-anchor=\"middle\" class=\"small\">attn.c_attn.weight</text>\n",
    "  <text x=\"0.25\" y=\"0.4833333333\" text-anchor=\"middle\" class=\"small\">attn.c_attn.bias</text>\n",
    "\n",
    "  <!-- Attention Computation -->\n",
    "  <rect x=\"0.0875\" y=\"0.5\" width=\"0.325\" height=\"0.0666666667\" fill=\"#FF69B4\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.25\" y=\"0.5416666667\" text-anchor=\"middle\" class=\"subcaption\">Attention Computation</text>\n",
    "\n",
    "  <!-- Output Projection -->\n",
    "  <rect x=\"0.0875\" y=\"0.5833333333\" width=\"0.325\" height=\"0.1\" fill=\"#FFD700\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.25\" y=\"0.625\" text-anchor=\"middle\" class=\"subcaption\">Output Projection</text>\n",
    "  <text x=\"0.25\" y=\"0.6583333333\" text-anchor=\"middle\" class=\"small\">attn.c_proj.weight</text>\n",
    "  <text x=\"0.25\" y=\"0.6833333333\" text-anchor=\"middle\" class=\"small\">attn.c_proj.bias</text>\n",
    "\n",
    "  <!-- Layer Norm 2 -->\n",
    "  <rect x=\"0.0625\" y=\"0.6833333333\" width=\"0.125\" height=\"0.1\" fill=\"#BAFFC9\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.125\" y=\"0.7333333333\" text-anchor=\"middle\" class=\"subcaption\">Layer Norm 2</text>\n",
    "  <text x=\"0.125\" y=\"0.7666666667\" text-anchor=\"middle\" class=\"small\">ln_2.weight</text>\n",
    "  <text x=\"0.125\" y=\"0.7916666667\" text-anchor=\"middle\" class=\"small\">ln_2.bias</text>\n",
    "\n",
    "  <!-- Feed Forward Network -->\n",
    "  <rect x=\"0.0625\" y=\"0.8166666667\" width=\"0.375\" height=\"0.1666666667\" fill=\"#FFA07A\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.25\" y=\"0.8583333333\" text-anchor=\"middle\" class=\"caption\">Feed Forward Network</text>\n",
    "  \n",
    "  <!-- First FC Layer -->\n",
    "  <rect x=\"0.0875\" y=\"0.8833333333\" width=\"0.15\" height=\"0.0833333333\" fill=\"#98FB98\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.1625\" y=\"0.925\" text-anchor=\"middle\" class=\"small\">mlp.c_fc.weight</text>\n",
    "  <text x=\"0.1625\" y=\"0.95\" text-anchor=\"middle\" class=\"small\">mlp.c_fc.bias</text>\n",
    "\n",
    "  <!-- Second FC Layer -->\n",
    "  <rect x=\"0.2625\" y=\"0.8833333333\" width=\"0.15\" height=\"0.0833333333\" fill=\"#98FB98\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.3375\" y=\"0.925\" text-anchor=\"middle\" class=\"small\">mlp.c_proj.weight</text>\n",
    "  <text x=\"0.3375\" y=\"0.95\" text-anchor=\"middle\" class=\"small\">mlp.c_proj.bias</text>\n",
    "\n",
    "  <!-- Output -->\n",
    "  <rect x=\"0.3125\" y=\"0.0833333333\" width=\"0.125\" height=\"0.0666666667\" fill=\"#FFB3BA\" stroke=\"black\" stroke-width=\"0.001667\" />\n",
    "  <text x=\"0.375\" y=\"0.125\" text-anchor=\"middle\" class=\"subcaption\">Output</text>\n",
    "\n",
    "  <!-- Arrows -->\n",
    "  <line x1=\"0.125\" y1=\"0.15\" x2=\"0.125\" y2=\"0.1833333333\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.125\" y1=\"0.2833333333\" x2=\"0.125\" y2=\"0.3166666667\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.25\" y1=\"0.65\" x2=\"0.25\" y2=\"0.6833333333\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.125\" y1=\"0.7833333333\" x2=\"0.125\" y2=\"0.8166666667\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.375\" y1=\"0.0833333333\" x2=\"0.375\" y2=\"0.05\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.375\" y1=\"0.05\" x2=\"0.0375\" y2=\"0.05\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.0375\" y1=\"0.05\" x2=\"0.0375\" y2=\"0.95\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "  <line x1=\"0.0375\" y1=\"0.95\" x2=\"0.0625\" y2=\"0.95\" stroke=\"black\" stroke-width=\"0.003333\" />\n",
    "</svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the Model with Zero Shot Inferencing: T-5\n",
    "\n",
    "Test the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device) \n",
    "\n",
    "output = tokenizer.decode(\n",
    "    original_model_t5.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model with Zero Shot Inferencing: GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Translate the following natural language question to First Order Logic (FOL). Please respond with only the FOL statement. Don't include additional text.\n",
      "\n",
      "Question: Can you spot a Mini Cooper on the left?\n",
      "\n",
      "FOL Query:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Ground truth:\n",
      "TypeOf(x, MiniCooper)^InitialLocation(x, NearLeft)\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "\n",
    "# test_questions = random.sample(list(zip(dataset['validation']['Question'], dataset['validation']['FOL Query'])), 1)\n",
    "\n",
    "\n",
    "question = dataset_fol['validation'][index]['Question']\n",
    "fol_gt = dataset_fol['validation'][index]['FOL Query']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Translate the following natural language question to First Order Logic (FOL). \\\n",
    "Please respond with only the FOL statement. Don't include additional text.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "FOL Query:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer_gpt2(prompt, return_tensors=\"pt\").input_ids.to(original_model_gpt2.device)\n",
    "output = original_model_gpt2.generate(input_ids, max_new_tokens=100, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "generated_text = tokenizer_gpt2.decode(output[0], skip_special_tokens=True)\n",
    "fol_model = generated_text.split(\"FOL Query:\")[-1].strip()\n",
    "        \n",
    "\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'Ground truth:\\n{fol_gt}')\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{fol_model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocess the Dialog-Summary Dataset\n",
    "\n",
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n",
    "\n",
    "Training prompt (dialogue):\n",
    "```\n",
    "Summarize the following conversation.\n",
    "\n",
    "    Chris: This is his part of the conversation.\n",
    "    Antje: This is her part of the conversation.\n",
    "    \n",
    "Summary: \n",
    "```\n",
    "\n",
    "Training response (summary):\n",
    "```\n",
    "Both Chris and Antje participated in the conversation.\n",
    "```\n",
    "\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89f0fb48720461b9f7ecee4e8231882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([460, 512]) torch.Size([460, 512])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15445e2d2d4c435d8cd6dfd62d8cb978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 512]) torch.Size([500, 512])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff6c5ffdb634a64a518378b64029630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512]) torch.Size([1000, 512])\n",
      "torch.Size([500, 512]) torch.Size([500, 512])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    # Define the start and end prompts for the summarization task\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    \n",
    "    # Create a list of prompts by combining start_prompt, dialogue, and end_prompt\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    \n",
    "    # Tokenize the prompts and store the input_ids in the example dictionary\n",
    "    example['input_ids'] = tokenizer(\n",
    "        prompt, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    \n",
    "    # Tokenize the summaries and store the input_ids as labels in the example dictionary\n",
    "    example['labels'] = tokenizer(\n",
    "        example[\"summary\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    \n",
    "    print(example['input_ids'].shape, example['labels'].shape)\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 different splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns from the tokenized datasets\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To save some time in the lab, you will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a03432319e45f89874270bf4b0ab9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c774f839b1de4857909734df2d4dac36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df36a6bc7bda43538c6765e7243f276d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda _, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check the shapes of all three parts of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model with the Preprocessed Dataset\n",
    "\n",
    "Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    no_cuda= True, # This forces CPU usage\n",
    "    use_cpu = True # This is an additional safeguard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model_t5,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start training process...\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-10 14:53:27,080] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2024-10-10 14:53:27,104] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /home/skb5969/.triton/autotune: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_name_from_huggingface = \"truocpham/flan-dialogue-summary-checkpoint\"\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(instruct_model_name_from_huggingface, torch_dtype=torch.bfloat16)\n",
    "# instruct_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The size of the downloaded instruct model is approximately 1GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1: I'm thinking of upgrading your software.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person2# also wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model_t5.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6c1c2eaf164faf8b411241d3cd2748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE metric is commonly used in natural language processing for evaluating text summarization and translation tasks.\n",
    "\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It was introduced by Chin-Yew Lin in 2004. The basic idea behind ROUGE is to compare an automatically generated summary or translation against a set of reference (human-written) summaries or translations.\n",
    "\n",
    "Here are the main types of ROUGE metrics:\n",
    "\n",
    "1. ROUGE-N:\n",
    "   This measures the overlap of n-grams between the generated and reference texts.\n",
    "   - ROUGE-1: Measures unigram overlap\n",
    "   - ROUGE-2: Measures bigram overlap\n",
    "   - ROUGE-3: Measures trigram overlap, and so on\n",
    "\n",
    "2. ROUGE-L:\n",
    "   This measures the longest common subsequence (LCS) between the generated and reference texts. It's more flexible than ROUGE-N as it allows for gaps in the match.\n",
    "\n",
    "3. ROUGE-W:\n",
    "   A weighted version of ROUGE-L that gives more importance to consecutive matches.\n",
    "\n",
    "4. ROUGE-S:\n",
    "   This measures the overlap of skip-bigrams between the generated and reference texts. Skip-bigrams are pairs of words in their sentence order, allowing for gaps between them.\n",
    "\n",
    "For each type of ROUGE, we typically calculate three scores:\n",
    "\n",
    "1. Precision: The ratio of the number of overlapping units to the total number of units in the generated text.\n",
    "2. Recall: The ratio of the number of overlapping units to the total number of units in the reference text.\n",
    "3. F1-score: The harmonic mean of precision and recall, providing a balanced measure.\n",
    "\n",
    "Here's a simple example to illustrate ROUGE-1:\n",
    "\n",
    "Reference: \"The cat sat on the mat.\"\n",
    "Generated: \"The cat lay on the rug.\"\n",
    "\n",
    "ROUGE-1 would count the overlapping unigrams:\n",
    "Overlapping: \"The\", \"cat\", \"on\", \"the\"\n",
    "Precision: 4/6 (4 overlapping words out of 6 in the generated text)\n",
    "Recall: 4/6 (4 overlapping words out of 6 in the reference text)\n",
    "F1-score: 2 * (4/6 * 4/6) / (4/6 + 4/6) = 0.667\n",
    "\n",
    "ROUGE scores range from 0 to 1, where higher scores indicate better performance.\n",
    "\n",
    "It's worth noting that while ROUGE is widely used, it has limitations. It focuses on lexical overlap and doesn't capture semantic similarity or factual correctness. Therefore, it's often used in conjunction with other metrics or human evaluation for a more comprehensive assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to make a memorandum of...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>Ms. Dawson, please send me a memo on the new p...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1: This memo is to be distributed to al...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>Person1: I'm finally here! #Person2: I'm final...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The driver is stuck in a traffic jam.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>Then there's the traffic jams.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>#Person1: Kate, you can't believe what happene...</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>#Porning #Porning #Porning #Porning #Porning #...</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Brian, I'm so sorry to hear of you.</td>\n",
       "      <td>Brian's birthday is coming. #Person1# invites ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Employees are required to make a memorandum of...   \n",
       "1  Ms. Dawson, please send me a memo on the new p...   \n",
       "2  #Person1: This memo is to be distributed to al...   \n",
       "3  Person1: I'm finally here! #Person2: I'm final...   \n",
       "4              The driver is stuck in a traffic jam.   \n",
       "5                     Then there's the traffic jams.   \n",
       "6  #Person1: Kate, you can't believe what happene...   \n",
       "7  #Porning #Porning #Porning #Porning #Porning #...   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9     #Person1#: Brian, I'm so sorry to hear of you.   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again. #Person1...  \n",
       "4  #Person2# got stuck in traffic again. #Person1...  \n",
       "5  #Person2# got stuck in traffic again. #Person1...  \n",
       "6  Masha and Hero are getting divorced. Kate can'...  \n",
       "7  Masha and Hero are getting divorced. Kate can'...  \n",
       "8  Masha and Hero are getting divorced. Kate can'...  \n",
       "9  Brian's birthday is coming. #Person1# invites ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model_t5.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.17051333563821996, 'rouge2': 0.04698412698412698, 'rougeL': 0.15834672773022485, 'rougeLsum': 0.1590422279445028}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4015906463624618, 'rouge2': 0.17568542724181807, 'rougeL': 0.2874569966059625, 'rougeLsum': 0.2886327613084294}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The results show substantial improvement in all ROUGE metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n",
      "rouge1: 18.82%\n",
      "rouge2: 10.43%\n",
      "rougeL: 13.70%\n",
      "rougeLsum: 13.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n",
    "\n",
    "PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained LoRA adapter emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 2 with 8.88 GB free memory\n"
     ]
    }
   ],
   "source": [
    "def get_available_device():\n",
    "    if torch.cuda.is_available():\n",
    "        # Get the number of available GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        \n",
    "        if num_gpus > 0:\n",
    "            # Get the GPU with the most free memory\n",
    "            free_memory = []\n",
    "            for i in range(num_gpus):\n",
    "                total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "                reserved_memory = torch.cuda.memory_reserved(i)\n",
    "                allocated_memory = torch.cuda.memory_allocated(i)\n",
    "                free_memory.append(total_memory - reserved_memory - allocated_memory)\n",
    "            \n",
    "            device_id = free_memory.index(max(free_memory))\n",
    "            device = torch.device(f'cuda:{device_id}')\n",
    "            print(f\"Using GPU {device_id} with {free_memory[device_id] / 1e9:.2f} GB free memory\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(\"No GPUs available. Using CPU.\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Use the function to get the best available device\n",
    "device = get_available_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning T-5 with `transformer.Trainer`\n",
    "The Trainer class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(original_model_t5, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n",
    "\n",
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Add LoRA adapter layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check that the size of this model is much less than the original LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning GPT-2 with `trl.SFTTrainer`\n",
    "\n",
    "The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including: Dataset formatting, including conversational and instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "lora_r = 8  # Reduced from 16 to 8 for GPT-2\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]  # GPT-2 specific target modules\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer..\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",  # GPT-2 doesn't use bias in attention layers\n",
    ")\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 1, meaning that the batch size for training and evaluation will be 4 per device.\n",
    "# 'gradient_accumulation_steps' is set to 8, meaning that gradients will be accumulated over 8 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"info\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 5e-5, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device and the model.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device and the model..\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 200, meaning that the model will be trained for 200 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"cosine\", meaning that a cosine learning rate scheduler will be used.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-LoRA-nl-to-fol-2\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=8, \n",
    "    per_device_eval_batch_size=1,  \n",
    "    log_level=\"info\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100, \n",
    "    learning_rate=5e-5, \n",
    "    fp16=True,  \n",
    "    eval_steps=100,  \n",
    "    num_train_epochs=200, \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "# 'model' is the model that will be trained.\n",
    "# 'train_dataset' and 'eval_dataset' are the datasets that will be used for training and evaluation, respectively.\n",
    "# 'peft_config' is the configuration for peft, which is used for instruction tuning.\n",
    "# 'dataset_text_field' is set to \"text\", meaning that the 'text' field of the dataset will be used as the input for the model.\n",
    "# 'max_seq_length' is set to 256, meaning that the maximum length of the sequences that will be fed to the model is 256 tokens.\n",
    "# 'tokenizer' is the tokenizer that will be used to tokenize the input text.\n",
    "# 'args' are the training arguments that were defined earlier.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256, \n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./gpt-2-LoRA-final\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = PeftModel.from_pretrained(model, \"./gpt-2-LoRA-final\")\n",
    "\n",
    "# Save the entire model (base GPT-2 + LoRA) to a single file\n",
    "fine_tuned_model = fine_tuned_model.merge_and_unload()\n",
    "fine_tuned_model.save_pretrained(\"./gpt-2-merged\")\n",
    "tokenizer.save_pretrained(\"./gpt-2-merged\")\n",
    "print(\"Merged model saved to: ./gpt-2-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can use the model saved in `./gpt-2-merged` folder to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing fine-tuned T-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model_name_from_huggingface = \"z7ye/peft-dialogue-summary-checkpoint\"\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       peft_model_name_from_huggingface, \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The number of trainable parameters will be `0` due to `is_trainable=False` setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      " all model parameters: 251116800\n",
      " percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model Qualitatively (Human Evaluation)\n",
    "\n",
    "Make inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person2# also wants to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1# recommends adding a painting program to #Person2#'s software and upgrading hardware. #Person2# also wants to upgrade the hardware because it's outdated now.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model_t5.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "Perform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1#: I need to take a dictation for this...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>New communication policy at the Office of the ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>This memo is for all employees.</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jams are a problem in the city.</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The public transport system is good for the en...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1#: I'm sorry for the traffic jam. #Per...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "      <td>#Person2# got stuck in traffic and #Person1# s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced. Kate can'...</td>\n",
       "      <td>Kate tells #Person2# Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>People are celebrating Brian's birthday.</td>\n",
       "      <td>Brian's birthday is coming. #Person1# invites ...</td>\n",
       "      <td>Brian remembers his birthday and invites #Pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  #Person1#: I need to take a dictation for this...   \n",
       "1  New communication policy at the Office of the ...   \n",
       "2                    This memo is for all employees.   \n",
       "3        The traffic jams are a problem in the city.   \n",
       "4  The public transport system is good for the en...   \n",
       "5  #Person1#: I'm sorry for the traffic jam. #Per...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7                       Masha and Hero are divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9           People are celebrating Brian's birthday.   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...   \n",
       "3  #Person2# got stuck in traffic again. #Person1...   \n",
       "4  #Person2# got stuck in traffic again. #Person1...   \n",
       "5  #Person2# got stuck in traffic again. #Person1...   \n",
       "6  Masha and Hero are getting divorced. Kate can'...   \n",
       "7  Masha and Hero are getting divorced. Kate can'...   \n",
       "8  Masha and Hero are getting divorced. Kate can'...   \n",
       "9  Brian's birthday is coming. #Person1# invites ...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic and #Person1# s...  \n",
       "4  #Person2# got stuck in traffic and #Person1# s...  \n",
       "5  #Person2# got stuck in traffic and #Person1# s...  \n",
       "6  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "7  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "8  Kate tells #Person2# Masha and Hero are gettin...  \n",
       "9  Brian remembers his birthday and invites #Pers...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model_t5.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ROUGE score for this subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2288514902063289, 'rouge2': 0.08646891938646062, 'rougeL': 0.2000912588009362, 'rougeLsum': 0.20380631316115186}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.4015906463624618, 'rouge2': 0.17568542724181807, 'rougeL': 0.2874569966059625, 'rougeLsum': 0.2886327613084294}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.3725351062275605, 'rouge2': 0.12138811933618107, 'rougeL': 0.27620639623170606, 'rougeLsum': 0.2758134870822362}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that PEFT model results are not too bad, while the training process was much easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\n",
      "INSTRUCT MODEL:\n",
      "{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "peft_model_summaries     = results['peft_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n",
    "\n",
    "Calculate the improvement of PEFT over the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 17.47%\n",
      "rouge2: 8.73%\n",
      "rougeL: 12.36%\n",
      "rougeLsum: 12.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the improvement of PEFT over a full fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\n",
      "rouge1: -1.35%\n",
      "rouge2: -1.70%\n",
      "rougeL: -1.34%\n",
      "rougeLsum: -1.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU)."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
